{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:53:16.904081Z",
     "start_time": "2018-11-30T02:53:14.083581Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read in data from dataset\n",
    "dataset = os.path.expanduser('~/data/question_pair_dataframe.csv')\n",
    "data = pd.read_csv(dataset,sep=',')\n",
    "data = data[0:400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:53:16.926075Z",
     "start_time": "2018-11-30T02:53:16.907123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why did your brown dogs have black puppies?</td>\n",
       "      <td>Who contributed to non-euclidean?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What age boy dogs able to create puppies?</td>\n",
       "      <td>What is the most recent processor for the desk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do earth formed according to science?</td>\n",
       "      <td>The science concered with earth and its place ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long is opened corked wine good for?</td>\n",
       "      <td>How long is wine good after it is opened?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People contribute in the development of computer?</td>\n",
       "      <td>Who are the key people in computer development?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the most recent desktop processor by amd?</td>\n",
       "      <td>Exclamation point horseshoe?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does key result areas?</td>\n",
       "      <td>How many calories is Coconut rum with Pineapple?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the function os vesicles?</td>\n",
       "      <td>What do vesicles function?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How is aerobic capacity expressed?</td>\n",
       "      <td>Average number of completed laps of aerobic ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does the yellow light mean when it turns ...</td>\n",
       "      <td>When did the new jersey devils win the stanley...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0        Why did your brown dogs have black puppies?   \n",
       "1          What age boy dogs able to create puppies?   \n",
       "2          How do earth formed according to science?   \n",
       "3           How long is opened corked wine good for?   \n",
       "4  People contribute in the development of computer?   \n",
       "5  What is the most recent desktop processor by amd?   \n",
       "6                        What does key result areas?   \n",
       "7                  What is the function os vesicles?   \n",
       "8                 How is aerobic capacity expressed?   \n",
       "9  What does the yellow light mean when it turns ...   \n",
       "\n",
       "                                           question2  labels  \n",
       "0                  Who contributed to non-euclidean?       0  \n",
       "1  What is the most recent processor for the desk...       0  \n",
       "2  The science concered with earth and its place ...       1  \n",
       "3          How long is wine good after it is opened?       1  \n",
       "4    Who are the key people in computer development?       1  \n",
       "5                       Exclamation point horseshoe?       0  \n",
       "6   How many calories is Coconut rum with Pineapple?       0  \n",
       "7                         What do vesicles function?       1  \n",
       "8  Average number of completed laps of aerobic ca...       1  \n",
       "9  When did the new jersey devils win the stanley...       0  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:53:39.439600Z",
     "start_time": "2018-11-30T02:53:16.927721Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Read in GloVe word embeddings\n",
    "embeddings_50d = \"../../word_embeddings/glove.6B/glove.6B.100d.text\"\n",
    "embeddings_100d = \"../../word_embeddings/glove.6B/glove.6B.100d.text\"\n",
    "embeddings_200d = \"../../word_embeddings/glove.6B/glove.6B.200d.text\"\n",
    "embeddings_300d = \"../../word_embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "embeddings = pd.read_table(embeddings_300d, delim_whitespace=True, index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing step below takes about:\n",
    "- 17m for 200,000 question pairs \n",
    "- 4h to complete for ~2.5 million question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:53:48.615866Z",
     "start_time": "2018-11-30T02:53:39.442857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocab. Found 17706 words. Longest sentence is 41 words.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = set()\n",
    "    max_sentence_length = 0    \n",
    "\n",
    "    # Put all words in the training corpus into a set\n",
    "    for (q1, q2) in zip(data.question1, data.question2):\n",
    "        q1words = tf.keras.preprocessing.text.text_to_word_sequence(q1)\n",
    "        q2words = tf.keras.preprocessing.text.text_to_word_sequence(q2)\n",
    "        max_sentence_length = max(max_question_length, max(len(q1words), len(q2words)))\n",
    "        [vocab.add(word) for word in q1words + q2words] \n",
    "        \n",
    "    # Add token for unknown words\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return (list(vocab), len(vocab), max_sentence_length)\n",
    "\n",
    "vocab, vocab_length, max_sentence_length = build_vocab(data)\n",
    "vocab.sort()\n",
    "\n",
    "print(f\"Built vocab. Found {vocab_length} words. Longest sentence is {max_sentence_length} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.108Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_embedding_matrix(vocab, embedding_dim):\n",
    "    # Add 1 extra spot for <UNK> token\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for index in range(1, len(vocab)):\n",
    "        try:\n",
    "            word = vocab[index]\n",
    "            vector = embeddings.loc[\"the\"]\n",
    "            embedding_matrix[index] = vector\n",
    "        except:\n",
    "            continue\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(vocab, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.111Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "def build_word_index(data):\n",
    "    def process_sentence(sentence):\n",
    "        word_sequence = text_to_word_sequence(str(sentence))\n",
    "        return np.pad([vocab.index(word) for word in word_sequence],\n",
    "                      (max_sentence_length - len(word_sequence), 0),\n",
    "                      mode=\"constant\")\n",
    "\n",
    "    q1s = [process_sentence(sentence) for sentence in data.question1]\n",
    "    q2s = [process_sentence(sentence) for sentence in data.question2]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"question1\": q1s,\n",
    "        \"question2\": q2s,\n",
    "        \"labels\": data.labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_word_index(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.113Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Lambda, Conv1D, MaxPooling1D, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "left_inputs = Input(shape=(max_sentence_length, ), dtype='int32')\n",
    "right_inputs = Input(shape=(max_sentence_length, ), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_length,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_sentence_length)\n",
    "embedding_left = embedding_layer(left_inputs)\n",
    "embedding_right = embedding_layer(right_inputs)\n",
    "\n",
    "# # Bi-gram convolution\n",
    "# bg_convolution_layer = Conv1D(\n",
    "#     filters=16,\n",
    "#     kernel_size=2,\n",
    "#     padding=\"same\",\n",
    "#     use_bias=True,\n",
    "#     activation=tf.nn.tanh)(embedded_left)\n",
    "# # bg_pooling = MaxPooling1D(pool_size = 2)(bg_convolution_layer)\n",
    "\n",
    "# Tri-gram convolution\n",
    "tg_convolution_layer = Conv1D(\n",
    "    filters=300,\n",
    "    kernel_size=3,\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    input_shape=(max_sentence_length, embedding_dim),\n",
    "    activation=tf.nn.tanh)(embedded_left)\n",
    "# tg_pooling = MaxPooling1D(pool_size = 3)(tg_convolution_layer)\n",
    "\n",
    "# 4-gram convolution\n",
    "fg_convolution_layer = Conv1D(\n",
    "    filters=300,\n",
    "    kernel_size=4,\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    input_shape=(max_sentence_length, embedding_dim),\n",
    "    activation=tf.nn.tanh)(embedded_left)\n",
    "# fg_pooling = MaxPooling1D(pool_size = 4)(fg_convolution_layer)\n",
    "\n",
    "sgram_convolution_layer = Conv1D(\n",
    "    filters=300,\n",
    "    kernel_size=5,\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    input_shape=(max_sentence_length, embedding_dim),\n",
    "    activation=tf.nn.tanh)(embedding_left)\n",
    "\n",
    "merged_left = Concatenate(axis=2)([tg_convolution_layer, fg_convolution_layer, sgram_convolution_layer, embedding_left]) \n",
    "merged_right = Concatenate(axis=2)([tg_convolution_layer, fg_convolution_layer, sgram_convolution_layer, embedding_right]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.118Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "n_hidden_units = 30\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "# Similarity metric for output\n",
    "malstm_distance = Lambda(\n",
    "    function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "    output_shape=lambda x: (x[0][0], 1))([left_out, right_out])\n",
    "\n",
    "lstm = LSTM(n_hidden_units)\n",
    "left_out = lstm(merged_left)\n",
    "right_out = lstm(merged_right)\n",
    "\n",
    "model = Model(inputs=[left_inputs, right_inputs], outputs=[malstm_distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.120Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "validation_set_size = round(0.20*len(df))\n",
    "training_set_size = len(df) - validation_set_size\n",
    "\n",
    "X = df[[\"question1\",\"question2\"]]\n",
    "Y = df[\"labels\"]\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X,Y,test_size=validation_set_size)\n",
    "\n",
    "X_train = {\n",
    "    'left': X_train.question1, \n",
    "    'right': X_train.question2\n",
    "}\n",
    "X_validation = {\n",
    "    'left': X_validation.question1, \n",
    "    'right': X_validation.question2\n",
    "}\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side],  maxlen=max_sentence_length)\n",
    "\n",
    "print(f\"Training set size: {len(X_train['left'])}\")\n",
    "print(f\"Validation set size: {len(X_validation['right'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-30T02:53:14.123Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta\n",
    "from time import time\n",
    "import datetime \n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.train.GradientDescentOptimizer(0.5), metrics=['accuracy'])\n",
    "\n",
    "start_time = time()\n",
    "trained = model.fit(\n",
    "    [X_train['left'], X_train['right']], \n",
    "    Y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=num_epoch,\n",
    "    validation_data=([X_validation['left'], X_validation['right']], Y_validation)\n",
    ")\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(num_epoch, datetime.timedelta(seconds=time()-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
