{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we take of downloading the files, extracting them and reading the first file into memory. Note that unless `overwrite` is set, the files are not re-downloaded or re-extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T20:49:45.382391Z",
     "start_time": "2018-11-24T20:49:45.371274Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting 40 data files.\n",
      "Downloading 0 files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "base_url = \"http://knowitall.cs.washington.edu/oqa/data/wikianswers/\"\n",
    "number_of_archive_parts = 39\n",
    "data_directory = os.path.expanduser('~/data/')\n",
    "overwrite = False\n",
    "\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "\n",
    "# Create list of files to fetch\n",
    "parts = [\n",
    "    f\"part-{str(part_num).zfill(5)}.gz\"\n",
    "    for part_num in range(0, number_of_archive_parts)\n",
    "]\n",
    "questions = \"questions.txt\"\n",
    "files = parts + [questions]\n",
    "\n",
    "print(f\"Requesting {len(files)} data files.\")\n",
    "if (overwrite == False):\n",
    "    files = list(filter(lambda file: not os.path.exists(data_directory + file), files))\n",
    "    print(f\"Downloading {len(files)} files.\")\n",
    "\n",
    "# Fetch WikiAnswer paraphrase corpus\n",
    "urls = [base_url + file for file in files]\n",
    "results = [\n",
    "    urllib.request.urlretrieve(url, data_directory + filename)\n",
    "    for (url, filename) in zip(urls, files)\n",
    "]\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T20:49:46.193487Z",
     "start_time": "2018-11-24T20:49:46.179886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 0 files.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def extract_files(files_to_extract):\n",
    "    # Check for previously extracted files and skip them if not overwriting\n",
    "    extracted_file_names = [os.path.splitext(file)[0] for file in files_to_extract]\n",
    "    if (overwrite == False):\n",
    "        files_to_extract = list(filter(lambda file: not os.path.exists(data_directory + file), extracted_file_names))\n",
    "        print(f\"Extracting {len(files_to_extract)} files.\")\n",
    "\n",
    "    for file in files_to_extract:\n",
    "        with gzip.open(data_directory + file + \".gz\", 'rb') as f_in:\n",
    "            with open(data_directory + file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    return extracted_file_names\n",
    "extracted_files = extract_files(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T00:39:53.614545Z",
     "start_time": "2018-11-25T00:39:08.303576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/erikbeerepoot/data/part-00038\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_line(line):\n",
    "    questions = [re.sub('q:','', question) for question in line.split('\\t') if question.find('a:') == -1]\n",
    "    return [re.sub('\\n','', question) for question in questions]\n",
    "\n",
    "file = data_directory + extracted_files[-1]\n",
    "print(file)\n",
    "with open(file) as f:\n",
    "    lines = [process_line(line) for line in f.readlines()]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the of the downloaded `.gz` files, there are sets of clustered questions. For example:\n",
    "\n",
    "```\n",
    "<SNIP>\n",
    "q:Calories in a handful of strawberries?\tq:Calories in handful of strawberries?\tq:How many calories are in 1cup of strawberrys?\tq:How many calories are in a handful of strawberries? \n",
    "</SNIP>\n",
    "```\n",
    "\n",
    "While the data are not perfect (some are more similar than others), it overall looks reasonable. We want to:\n",
    "1. Read in each of the clusters, and generate pairs of similar questions\n",
    "2. Randomly sample from other clusters to generate pairs of dissimilar questions \n",
    "3. Train our model on this dataset\n",
    "\n",
    "In order to maintain a balanced dataset, we want to create as many negative examples as there are postive examples. Hence, we compute the number of permutations (question pairs) we obtain and generate the same number of non-similar pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T00:50:19.217583Z",
     "start_time": "2018-11-25T00:49:28.638604Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def generate_question_pairs(cluster):\n",
    "    \"\"\"\n",
    "    Generate permutations of length to (i.e. question pairs) \n",
    "    \"\"\"\n",
    "    return list(itertools.permutations(cluster, 2))\n",
    "\n",
    "\n",
    "def generate_uniform_random_indices(low, high, count):\n",
    "    \"\"\"\n",
    "    Returns a list of integers using the discrete uniform distribution, without repeats.\n",
    "    low -- Starting index\n",
    "    high -- Ending index\n",
    "    count -- The number of integers to generate \n",
    "    \"\"\"\n",
    "    random_integers = []\n",
    "    generated_count = 0\n",
    "    while (generated_count < count):\n",
    "        # Generate a random integer that doesn't match [forbidden]\n",
    "        random_integer = np.random.randint(0, count)\n",
    "        while (random_integer in random_integers):\n",
    "            random_integer = np.random.randint(0, count)\n",
    "        random_integers.append(random_integer)\n",
    "        generated_count = generated_count + 1\n",
    "    return random_integers\n",
    "\n",
    "\n",
    "def assemble_distinct_question_pairs(current_cluster_index, num_to_generate,\n",
    "                                     all_clusters):\n",
    "    \"\"\"\n",
    "    Returns a list of dissimilar questions (i.e. negative examples)\n",
    "    current_cluster_index -- The index of the currrent cluster we're processing\n",
    "    num_to_generate -- The number of distinct pairs to generate\n",
    "    all_clusters -- The set of all clusters of questions\n",
    "    \"\"\"\n",
    "    number_of_clusters = len(all_clusters)\n",
    "    indices = generate_uniform_random_indices(0, number_of_clusters,\n",
    "                                              num_to_generate)\n",
    "\n",
    "    questions = []\n",
    "    for index in indices:\n",
    "        cluster_size = len(all_clusters[index])\n",
    "        if(cluster_size > 0):\n",
    "            questions.append(all_clusters[index][np.random.randint(0, len(all_clusters[index]))])\n",
    "            \n",
    "    return generate_question_pairs(questions)\n",
    "\n",
    "\n",
    "# Generate question pairs for both semantically similar questions and distinct (dissimilar) questions\n",
    "similar_question_pairs = []\n",
    "distinct_question_pairs = []\n",
    "\n",
    "foo = lines[0:1500]\n",
    "for line in foo:\n",
    "    number_of_distinct_phrasings = len(line)\n",
    "    cluster_index = lines.index(line)\n",
    "    question_pairs = generate_question_pairs(line)\n",
    "\n",
    "    similar_question_pairs = similar_question_pairs + question_pairs\n",
    "    distinct_question_pairs = distinct_question_pairs + assemble_distinct_question_pairs(\n",
    "        cluster_index, number_of_distinct_phrasings, lines\n",
    "    )\n",
    "\n",
    "num_similar_question_pairs = sum(\n",
    "    list([len(foo) for foo in similar_question_pairs]))\n",
    "num_distinct_question_pairs = sum(\n",
    "    list([len(foo) for foo in distinct_question_pairs]))\n",
    "assert (num_similar_question_pairs == num_distinct_question_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the question pairs, we can assemble them into our input dataframe & write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T00:50:46.861063Z",
     "start_time": "2018-11-25T00:50:36.644164Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out_path = os.path.expanduser('~/data/question_pair_dataframe.csv')\n",
    "\n",
    "similar_df = pd.DataFrame(\n",
    "    similar_question_pairs[:], columns=['question1', 'question2'])\n",
    "similar_df.insert(2, \"labels\", 1)\n",
    "\n",
    "distinct_df = pd.DataFrame(\n",
    "    distinct_question_pairs[:], columns=['question1', 'question2'])\n",
    "distinct_df.insert(2, \"labels\", 0)\n",
    "\n",
    "combined_df = pd.concat([similar_df, distinct_df], axis=0, ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "combined_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
